# üöÄ Deployment Guide: Snowflake Semantic Model Pipeline

This guide helps you deploy the complete Snowflake semantic model pipeline for a new user or environment.

## üìã Prerequisites Checklist

Before starting, ensure you have:

- [ ] **Snowflake Account** with admin access
- [ ] **Python 3.7+** installed
- [ ] **Split CSV files** ready for upload
- [ ] **Semantic model YAML** file
- [ ] **Network access** to Snowflake from your machine

## üéØ One-Command Deployment

### Step 1: Download/Clone Files

Create a new directory and copy these essential files:

```
your-project/
‚îú‚îÄ‚îÄ config.py                    # ‚Üê EDIT THIS FILE
‚îú‚îÄ‚îÄ setup_pipeline.py            # ‚Üê RUN THIS SCRIPT
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ semantic_model.yaml          # Your semantic model definition
‚îú‚îÄ‚îÄ split_files/                 # Your CSV files directory
‚îÇ   ‚îú‚îÄ‚îÄ anon_views_part_01.csv
‚îÇ   ‚îú‚îÄ‚îÄ anon_views_part_02.csv
‚îÇ   ‚îú‚îÄ‚îÄ anon_user_day_fact.csv
‚îÇ   ‚îî‚îÄ‚îÄ anon_company_day_fact.csv
‚îî‚îÄ‚îÄ scripts/                     # Supporting scripts
    ‚îú‚îÄ‚îÄ load_split_files.py
    ‚îú‚îÄ‚îÄ verify_data_integrity.py
    ‚îú‚îÄ‚îÄ create_semantic_model_from_yaml.py
    ‚îî‚îÄ‚îÄ check_split_files.py
```

### Step 2: Configure Credentials

Edit `config.py` and update ONLY these values:

```python
# EDIT THESE VALUES FOR YOUR ENVIRONMENT
SNOWFLAKE_ACCOUNT = "your-account-id"        # e.g., "abc12345"
SNOWFLAKE_USER = "your-username"             # e.g., "john_doe"  
SNOWFLAKE_PASSWORD = "your-password"         # e.g., "MyPassword123"
SNOWFLAKE_WAREHOUSE = "COMPUTE_WH"           # Your warehouse name
SNOWFLAKE_DATABASE = "YOUR_DATABASE"         # e.g., "ANALYTICS_DB"
SNOWFLAKE_SCHEMA = "PUBLIC"                  # Schema name
SNOWFLAKE_ROLE = "ACCOUNTADMIN"              # Your role

# UPDATE IF YOUR FILES ARE IN DIFFERENT LOCATIONS
SPLIT_FILES_DIRECTORY = "split_files"        # Directory with CSV files
SEMANTIC_MODEL_YAML = "semantic_model.yaml"  # Path to YAML file
```

### Step 3: Run One Command

```bash
python setup_pipeline.py
```

**That's it!** The script will:
- ‚úÖ Validate your configuration
- ‚úÖ Install required Python packages
- ‚úÖ Check your split files
- ‚úÖ Connect to Snowflake
- ‚úÖ Create database and schema
- ‚úÖ Load all CSV data
- ‚úÖ Verify data integrity
- ‚úÖ Create semantic views
- ‚úÖ Generate Snowsight instructions

## üìä Expected Output

### Successful Run Output:
```
üöÄ Snowflake Semantic Model Pipeline
============================================================

üìã STEP 1/7: Validating Prerequisites
--------------------------------------------------
‚úÖ Configuration is valid!

üìã STEP 2/7: Installing Requirements
--------------------------------------------------
‚úÖ All packages installed successfully!

üìã STEP 3/7: Checking Split Files
--------------------------------------------------
‚úÖ Split files check completed!

üìã STEP 4/7: Loading Data into Snowflake
--------------------------------------------------
üìä Data Loading Results:
  - ANON_VIEWS: 2,500,000 rows loaded
  - ANON_USER_DAY_FACT: 722,374 rows loaded
  - ANON_COMPANY_DAY_FACT: 331,093 rows loaded
‚úÖ Total rows loaded: 3,553,467

üìã STEP 5/7: Verifying Data Integrity
--------------------------------------------------
‚úÖ 3/3 tables have perfect integrity

üìã STEP 6/7: Creating Semantic Model Infrastructure
--------------------------------------------------
‚úÖ Semantic model infrastructure created!

üìã STEP 7/7: Generating Final Instructions
--------------------------------------------------
üéØ SNOWSIGHT SETUP INSTRUCTIONS:
============================================================

üéâ PIPELINE COMPLETED SUCCESSFULLY!
============================================================
```

## üîß Manual Deployment (Alternative)

If you prefer step-by-step control:

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Test Configuration
```bash
python config.py
```

### 3. Check Files
```bash
python scripts/check_split_files.py
```

### 4. Load Data
```bash
python scripts/load_split_files.py
```

### 5. Verify Data
```bash
python scripts/verify_data_integrity.py
```

### 6. Create Semantic Model
```bash
python scripts/create_semantic_model_from_yaml.py
```

## üéØ Snowsight Final Setup

After successful pipeline execution:

### 1. Access Snowsight
- URL: `https://your-account.snowflakecomputing.com`
- Login with your credentials

### 2. Create Semantic Model
- Navigate to **Data** > **Semantic Models**
- Click **"Create Semantic Model"**
- Model Name: `PAID_APP_PLATFORM_ANON`

### 3. Add Base Tables
- `SEMANTIC_ANON_VIEWS` (2.5M rows)
- `SEMANTIC_ANON_USER_DAY_FACT` (722K rows)
- `SEMANTIC_ANON_COMPANY_DAY_FACT` (331K rows)

### 4. Configure Relationships
- Connect using `USER_ID`, `COMPANY_ID`, and `VIEW_DATE`

### 5. Test Natural Language
- "Show me total revenue by company status"
- "What is the average view time per user?"
- "How many paying customers viewed apps last month?"

## üö® Troubleshooting

### Common Issues & Solutions:

**‚ùå "Connection failed"**
```
Solution: Check your Snowflake credentials in config.py
- Verify account identifier (without .snowflakecomputing.com)
- Ensure warehouse is running
- Check network connectivity
```

**‚ùå "Split files not found"**
```
Solution: Verify file locations
- Check SPLIT_FILES_DIRECTORY path in config.py
- Ensure CSV files exist in the directory
- Verify file naming patterns match expected format
```

**‚ùå "Column mapping errors"**
```
Solution: Update column mappings in config.py
- Check COLUMN_MAPPINGS dictionary
- Verify CSV column names match expectations
- Update mappings for your specific data format
```

**‚ùå "Permission denied"**
```
Solution: Check Snowflake permissions
- Ensure your role has CREATE DATABASE privileges
- Verify warehouse usage permissions
- Check schema creation rights
```

### Getting Detailed Logs:

The pipeline creates detailed log files:
```bash
# Check the latest log file
ls -la pipeline_*.log

# View the log
cat pipeline_20240809_182345.log
```

## üìù Customization for Different Data

### For Different CSV Structure:

1. **Update column mappings** in `config.py`:
```python
COLUMN_MAPPINGS = {
    'YOUR_TABLE': {
        'csv_column_name': 'SNOWFLAKE_COLUMN_NAME',
        # Add your mappings here
    }
}
```

2. **Update table mappings**:
```python
TABLE_MAPPINGS = {
    'your_csv_file': 'YOUR_SNOWFLAKE_TABLE',
    # Add your mappings here
}
```

3. **Modify semantic model YAML** with your schema definition

### For Different Snowflake Setup:

1. **Update database/schema names** in `config.py`
2. **Change warehouse settings** as needed
3. **Modify role permissions** if required

## ‚úÖ Success Validation

Your deployment is successful when:

- [ ] All CSV files loaded without errors
- [ ] Row counts match between local files and Snowflake
- [ ] No critical data integrity issues
- [ ] Semantic views created successfully
- [ ] Snowsight instructions generated
- [ ] Natural language queries work in Snowflake

## üìû Support

If you encounter issues:

1. **Check the generated log files** for detailed error messages
2. **Run individual scripts** to isolate problems
3. **Verify prerequisites** are met
4. **Review configuration** settings
5. **Test Snowflake connectivity** separately

## üéâ Next Steps

After successful deployment:

1. **Explore your data** using Snowsight
2. **Create dashboards** with your semantic model
3. **Train users** on natural language queries
4. **Set up monitoring** for data freshness
5. **Plan regular data updates** if needed

---

**üöÄ You're now ready to deploy the Snowflake semantic model pipeline in any environment!**